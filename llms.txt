# Project: llmstxt

## Project Structure
This file contains the compressed and processed contents of the project.

### File Types
The following file types are included:
- .py
- .js
- .html
- .css
- .java
- .c
- .cpp
- .h
- .hpp
- .sh
- .txt
- .md
- .json
- .xml
- .yaml
- .yml
- .toml
- .ini

### Special Files
<file>README.md</file>
<metadata>
path: README.md
size: 5567 bytes
</metadata>

[![CodeQL Advanced](https://github.com/ngmisl/llmstxt/actions/workflows/codeql.yml/badge.svg)](https://github.com/ngmisl/llmstxt/actions/workflows/codeql.yml)
[![Update llms.txt](https://github.com/ngmisl/llmstxt/actions/workflows/update-llms.yml/badge.svg)](https://github.com/ngmisl/llmstxt/actions/workflows/update-llms.yml)

# llmstxt

A Python tool for compressing and organizing code files into a single, LLM-friendly text file. This tool is designed to help prepare codebases for analysis by Large Language Models by removing unnecessary content while preserving important semantic information.

## Features

### Smart Code Compression

- Preserves docstrings and important comments
- Removes redundant whitespace and formatting
- Maintains code structure and readability
- Handles multiple programming languages

### Language Support

- Python (with AST-based compression)
- JavaScript
- Java
- C/C++
- Shell scripts
- HTML/CSS
- Configuration files (JSON, YAML, TOML, INI)
- Markdown

### LLM-Friendly Output

- XML-style semantic markers
- File metadata and type information
- Organized imports section
- Clear file boundaries
- Consistent formatting

### Automation

- GitHub Actions integration
- Automatic updates on code changes
- CI/CD friendly

## Installation

This project uses [uv](https://github.com/astral-sh/uv) for dependency management, but can also be installed directly with pip.

```bash
# Using pip
pip install git+https://github.com/ngmisl/llmstxt.git

# Using uv (recommended for development)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv pip install .

# For development
uv pip install -e ".[dev]"
```

## Usage

### Local Usage

```bash
# Generate llms.txt from current directory
python -m llmstxt

# Or import and use in your code
from llmstxt import generate_llms_txt
generate_llms_txt()
```

The script will:

1. Scan the current directory recursively
2. Process files according to .gitignore rules
3. Generate `llms.txt` with compressed content

### GitHub Actions Integration

There are two ways to use this tool with GitHub Actions:

1. **For Your Own Repository**

Create `.github/workflows/update-llms.yml` with:

```yaml
name: Update llms.txt

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch: # Allow manual triggering

permissions:
  contents: write

jobs:
  update-llms:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install llmstxt tool
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install git+https://github.com/ngmisl/llmstxt.git

      - name: Generate llms.txt
        run: |
          . .venv/bin/activate
          rm -f llms.txt
          python -c "from llmstxt import generate_llms_txt; generate_llms_txt()"

      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

      - name: Commit and push changes
        run: |
          git add llms.txt
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update llms.txt"
            git push
          fi
```

The workflow will:

- Run on push to main/master
- Run on pull requests
- Can be triggered manually
- Generate and commit `llms.txt` automatically

2. **For Remote Repositories**
   You can trigger the action for any repository using the GitHub API:

```bash
curl -X POST \
  -H "Authorization: token $GITHUB_TOKEN" \
  -H "Accept: application/vnd.github.v3+json" \
  https://api.github.com/repos/ngmisl/llmstxt/dispatches \
  -d '{"event_type": "update-llms", "client_payload": {"repository": "https://github.com/user/repo.git"}}'
```

## Output Format

The generated `llms.txt` file follows this structure:

```python
# Project: llmstxt

## Project Structure
This file contains the compressed and processed contents of the project.

### File Types
- .py
- .js
- .java
...

<file>src/main.py</file>
<metadata>
path: src/main.py
type: py
size: 1234 bytes
</metadata>

<imports>
import ast
from typing import Optional
</imports>

<code lang='python'>
def example():
    """Docstring preserved."""
    return True
</code>

<file>src/utils.js</file>
<metadata>
path: src/utils.js
type: js
size: 567 bytes
</metadata>

<code lang='javascript'>
function helper() {
  return true;
}
</code>
```

## Configuration

The tool can be configured through function parameters:

```python
generate_llms_txt(
    output_file="llms.txt",      # Output filename
    max_file_size=100 * 1024,    # Max file size (100KB)
    allowed_extensions=(         # Supported file types
        ".py", ".js", ".java",
        ".c", ".cpp", ".h", ".hpp",
        ".sh", ".txt", ".md",
        ".json", ".xml", ".yaml",
        ".yml", ".toml", ".ini"
    )
)
```

## Development

Requirements:

- Python 3.8+
- [uv](https://github.com/astral-sh/uv) for dependency management (recommended)

```bash
# Clone the repository
git clone https://github.com/ngmisl/llmstxt.git
cd llmstxt

# Install development dependencies
uv pip install -e ".[dev]"

# Run type checking
mypy llmstxt

# Run linting and formatting
ruff check llmstxt
ruff format llmstxt
```

## License

MIT License - See LICENSE file for details


<file>LICENSE</file>
<metadata>
path: LICENSE
size: 1063 bytes
</metadata>

MIT License

Copyright (c) 2024 ngmisl

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


<file>pyproject.toml</file>
<metadata>
path: pyproject.toml
type: toml
size: 659 bytes
</metadata>

<content type='toml'>
[project]
name = "llmstxt"
version = "0.1.0"
description = "Compress code files into LLM-friendly format"
authors = [{ name = "Aunova", email = "chris@aunova.net" }]
dependencies = ["gitignore_parser>=0.1.11", "astroid>=3.0.1"]
requires-python = ">=3.8"
readme = "README.md"
license = { text = "MIT" }

[project.optional-dependencies]
dev = ["mypy>=1.7.0", "ruff>=0.1.8"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build]
packages = ["llmstxt"]

[tool.ruff]
line-length = 88
target-version = "py38"

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

</content>

<file>renovate.json</file>
<metadata>
path: renovate.json
type: json
size: 114 bytes
</metadata>

<content type='json'>
{
  "$schema": "https://docs.renovatebot.com/renovate-schema.json",
  "extends": [
    "config:recommended"
  ]
}

</content>

<file>llmstxt/__init__.py</file>
<metadata>
path: llmstxt/__init__.py
type: py
size: 154 bytes
</metadata>

<imports>
from .llms import generate_llms_txt
</imports>

<code lang='py'>
"""llmstxt - Compress code files into LLM-friendly format."""
from .llms import generate_llms_txt
__version__ = '0.1.0'
__all__ = ['generate_llms_txt']
</code>

<file>llmstxt/__main__.py</file>
<metadata>
path: llmstxt/__main__.py
type: py
size: 131 bytes
</metadata>

<imports>
from .llms import generate_llms_txt
</imports>

<code lang='py'>
"""Command-line interface for llmstxt."""
from .llms import generate_llms_txt
if __name__ == '__main__':
    generate_llms_txt()
</code>

<file>llmstxt/llms.py</file>
<metadata>
path: llmstxt/llms.py
type: py
size: 12329 bytes
</metadata>

<imports>
import ast
import pathlib
import re
from typing import Optional, Sequence, cast
import astroid  # type: ignore
from gitignore_parser import parse_gitignore  # type: ignore
    """Removes multiple blank lines from text files."""
    """Extracts code blocks from Markdown and compresses them."""
        outfile.write("The following file types are included:\n")
        # Include README and LICENSE with metadata
                                        "import ",
                                        "from ",
                                        "require",
                                        "include",
</imports>

<code lang='py'>
import ast
import pathlib
import re
from typing import Optional, Sequence, cast
import astroid
from gitignore_parser import parse_gitignore

def compress_python_code(content: str) -> str:
    """Compress Python code while preserving docstrings."""
    try:
        parsed_ast = ast.parse(content)
    except SyntaxError:
        return content

    class RemoveCommentsAndDocstrings(ast.NodeTransformer):

        def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.FunctionDef]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def visit_ClassDef(self, node: ast.ClassDef) -> Optional[ast.ClassDef]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def visit_Module(self, node: ast.Module) -> Optional[ast.Module]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def generic_visit(self, node: ast.AST) -> ast.AST:
            if isinstance(node, ast.Expr) and isinstance(node.value, ast.Constant):
                return ast.Pass()
            return super().generic_visit(node)
    try:
        transformer = RemoveCommentsAndDocstrings()
        cleaned_ast = transformer.visit(parsed_ast)
        ast.fix_missing_locations(cleaned_ast)
        try:
            source = ast.unparse(cleaned_ast)
        except (AttributeError, TypeError):
            source = cast(str, astroid.parse(ast.dump(cleaned_ast)).as_string())
        lines = source.split('\n')
        cleaned_lines = []
        for line in lines:
            if line.strip() != 'pass':
                cleaned_lines.append(line)
        compressed_code = '\n'.join(cleaned_lines)
        compressed_code = re.sub('\\n\\s*\\n\\s*\\n', '\n\n', compressed_code)
        return compressed_code
    except Exception as e:
        print(f'Warning: Error compressing Python code: {e}')
        return content

def compress_code_content(content: str, file_extension: str) -> str:
    """Compress code content based on the file extension."""
    if file_extension in ('.py', '.pyi'):
        return compress_python_code(content)
    return basic_compress(content, file_extension)

def basic_compress(content: str, file_extension: str) -> str:
    """Basic compression: remove comments and multiple blank lines."""
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if file_extension in ('.py', '.sh'):
            line = re.sub('#.*$', '', line)
        elif file_extension in ('.js', '.java', '.c', '.cpp', '.h', '.hpp'):
            line = re.sub('//.*$', '', line)
        cleaned_lines.append(line)
    content = '\n'.join(cleaned_lines)
    return re.sub('\\n\\s*\\n\\s*\\n', '\n\n', content)

def compress_text_content(content: str) -> str:
    """Removes multiple blank lines from text files."""
    return re.sub('\\n\\s*\\n\\s*\\n', '\n\n', content)

def compress_markdown_content(content: str) -> str:
    """Extracts code blocks from Markdown and compresses them."""
    parts = re.split('(```\\w*\\n.*?\\n```)', content, flags=re.DOTALL)
    compressed_parts = []
    for part in parts:
        if part.startswith('```'):
            lang_match = re.match('```(\\w*)\\n', part)
            lang = lang_match.group(1) if lang_match else ''
            code = re.sub('```\\w*\\n(.*)\\n```', '\\1', part, flags=re.DOTALL)
            if lang in ('python', 'py'):
                code = compress_python_code(code)
            else:
                code = compress_text_content(code)
            compressed_parts.append(f'```{lang}\n{code}\n```')
        else:
            compressed_parts.append(compress_text_content(part))
    return ''.join(compressed_parts)

def generate_llms_txt(output_file: str='llms.txt', allowed_extensions: Sequence[str]=('.py', '.js', '.html', '.css', '.java', '.c', '.cpp', '.h', '.hpp', '.sh', '.txt', '.md', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini'), max_file_size: int=100 * 1024) -> None:
    """
    Generates a compressed llms.txt file optimized for LLM/AI consumption.

    Args:
        output_file: Name of the output file
        allowed_extensions: Tuple of file extensions to process
        max_file_size: Maximum file size in bytes to process
    """
    current_dir: pathlib.Path = pathlib.Path('.')
    gitignore_path: pathlib.Path = current_dir / '.gitignore'
    matches = parse_gitignore(gitignore_path) if gitignore_path.exists() else None
    with open(output_file, 'w', encoding='utf-8') as outfile:
        outfile.write('# Project: llmstxt\n\n')
        outfile.write('## Project Structure\n')
        outfile.write('This file contains the compressed and processed contents of the project.\n\n')
        outfile.write('### File Types\n')
        outfile.write('The following file types are included:\n')
        outfile.write(''.join([f'- {ext}\n' for ext in allowed_extensions]))
        outfile.write('\n### Special Files\n')
        for special_file in ['README.md', 'LICENSE', 'LICENSE.txt']:
            special_path: pathlib.Path = current_dir / special_file
            if special_path.exists():
                outfile.write(f'<file>{special_file}</file>\n')
                outfile.write('<metadata>\n')
                outfile.write(f'path: {special_file}\n')
                outfile.write(f'size: {special_path.stat().st_size} bytes\n')
                outfile.write('</metadata>\n\n')
                with open(special_path, 'r', encoding='utf-8', errors='replace') as infile:
                    special_content: str = infile.read()
                    outfile.write(special_content + '\n\n')
        for file in current_dir.rglob('*'):
            if file.is_file() and file.suffix.lower() in allowed_extensions and (not (matches and matches(str(file.relative_to(current_dir))))) and (file.name not in ['README.md', 'LICENSE', 'LICENSE.txt', output_file]):
                if file.stat().st_size > max_file_size:
                    print(f'Skipping {file} as it exceeds the maximum file size.')
                    continue
                relative_path: pathlib.Path = file.relative_to(current_dir)
                outfile.write(f'<file>{relative_path}</file>\n')
                outfile.write('<metadata>\n')
                outfile.write(f'path: {relative_path}\n')
                outfile.write(f'type: {file.suffix.lstrip('.')}\n')
                outfile.write(f'size: {file.stat().st_size} bytes\n')
                outfile.write('</metadata>\n\n')
                try:
                    with open(file, 'r', encoding='utf-8', errors='replace') as infile:
                        raw_content: str = infile.read()
                        if file.suffix.lower() in ('.py', '.js', '.java'):
                            outfile.write('<imports>\n')
                            import_lines = [line for line in raw_content.split('\n') if any((imp in line.lower() for imp in ['import ', 'from ', 'require', 'include']))]
                            if import_lines:
                                outfile.write('\n'.join(import_lines) + '\n')
                            outfile.write('</imports>\n\n')
                        if file.suffix.lower() in ('.py', '.js', '.java', '.c', '.cpp', '.h', '.hpp', '.sh'):
                            code_content: str = compress_code_content(raw_content, file.suffix.lower())
                            language: str = file.suffix.lstrip('.')
                            outfile.write(f"<code lang='{language}'>\n{code_content}\n</code>\n\n")
                        elif file.suffix.lower() in ('.txt', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini'):
                            text_content: str = compress_text_content(raw_content)
                            outfile.write(f"<content type='{file.suffix.lstrip('.')}'>\n")
                            outfile.write(f'{text_content}\n')
                            outfile.write('</content>\n\n')
                        elif file.suffix.lower() == '.md':
                            md_content: str = compress_markdown_content(raw_content)
                            outfile.write('<markdown>\n')
                            outfile.write(f'{md_content}\n')
                            outfile.write('</markdown>\n\n')
                        else:
                            outfile.write(f"<content type='{file.suffix.lstrip('.')}'>\n")
                            outfile.write(f'{raw_content}\n')
                            outfile.write('</content>\n\n')
                except Exception as e:
                    outfile.write(f'<error>Error processing {relative_path}: {e}</error>\n\n')
if __name__ == '__main__':
    output_filename: str = 'llms.txt'
    generate_llms_txt(output_filename)
    print(f'{output_filename} generated successfully in the current directory!')
</code>

<file>.vscode/settings.json</file>
<metadata>
path: .vscode/settings.json
type: json
size: 527 bytes
</metadata>

<content type='json'>
{
  "cSpell.words": [
    "Aunova",
    "celerybeat",
    "Connor",
    "cython",
    "direnv",
    "dmypy",
    "docstrings",
    "htmlcov",
    "infile",
    "ipynb",
    "isort",
    "llms",
    "llmstxt",
    "mccabe",
    "mkdocs",
    "mypy",
    "nosetests",
    "Pipfile",
    "pybuilder",
    "pycache",
    "pyflow",
    "pypa",
    "pypackages",
    "pyrightconfig",
    "pytest",
    "pytype",
    "ropeproject",
    "Scrapy",
    "sdist",
    "Spyder",
    "spyderproject",
    "spyproject",
    "webassets"
  ]
}

</content>

<file>.github/workflows/codeql.yml</file>
<metadata>
path: .github/workflows/codeql.yml
type: yml
size: 4301 bytes
</metadata>

<content type='yml'>
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL Advanced"

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  schedule:
    - cron: '26 7 * * 5'

jobs:
  analyze:
    name: Analyze (${{ matrix.language }})
    # Runner size impacts CodeQL analysis time. To learn more, please see:
    #   - https://gh.io/recommended-hardware-resources-for-running-codeql
    #   - https://gh.io/supported-runners-and-hardware-resources
    #   - https://gh.io/using-larger-runners (GitHub.com only)
    # Consider using larger runners or machines with greater resources for possible analysis time improvements.
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      # required for all workflows
      security-events: write

      # required to fetch internal or private CodeQL packs
      packages: read

      # only required for workflows in private repositories
      actions: read
      contents: read

    strategy:
      fail-fast: false
      matrix:
        include:
        - language: python
          build-mode: none
        # CodeQL supports the following values keywords for 'language': 'c-cpp', 'csharp', 'go', 'java-kotlin', 'javascript-typescript', 'python', 'ruby', 'swift'
        # Use `c-cpp` to analyze code written in C, C++ or both
        # Use 'java-kotlin' to analyze code written in Java, Kotlin or both
        # Use 'javascript-typescript' to analyze code written in JavaScript, TypeScript or both
        # To learn more about changing the languages that are analyzed or customizing the build mode for your analysis,
        # see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning.
        # If you are analyzing a compiled language, you can modify the 'build-mode' for that language to customize how
        # your codebase is analyzed, see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/codeql-code-scanning-for-compiled-languages
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}
        build-mode: ${{ matrix.build-mode }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality

    # If the analyze step fails for one of the languages you are analyzing with
    # "We were unable to automatically build your code", modify the matrix above
    # to set the build mode to "manual" for that language. Then modify this step
    # to build your code.
    # ℹ️ Command-line programs to run using the OS shell.
    # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun
    - if: matrix.build-mode == 'manual'
      shell: bash
      run: |
        echo 'If you are using a "manual" build mode for one or more of the' \
          'languages you are analyzing, replace this with the commands to build' \
          'your code, for example:'
        echo '  make bootstrap'
        echo '  make release'
        exit 1

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: "/language:${{matrix.language}}"

</content>

<file>.github/workflows/update-llms.yml</file>
<metadata>
path: .github/workflows/update-llms.yml
type: yml
size: 1403 bytes
</metadata>

<content type='yml'>
name: Update llms.txt

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch: # Allow manual triggering

permissions:
  contents: write

jobs:
  update-llms:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install llmstxt tool
        run: |
          python -m venv .venv
          . .venv/bin/activate
          python -m pip install --upgrade pip
          pip install git+https://github.com/ngmisl/llmstxt.git

      - name: Generate llms.txt
        run: |
          . .venv/bin/activate
          # Remove existing llms.txt if it exists
          rm -f llms.txt
          # Generate new llms.txt
          python -c "from llmstxt import generate_llms_txt; generate_llms_txt()"

      - name: Configure Git
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

      - name: Commit and push changes
        run: |
          git add llms.txt
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update llms.txt"
            git push
          fi

</content>

