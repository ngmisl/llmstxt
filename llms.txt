# Project: llmstxt

## Project Structure
This file contains the compressed and processed contents of the project.

### File Types
The following file types are included:
- .py
- .js
- .html
- .css
- .java
- .c
- .cpp
- .h
- .hpp
- .sh
- .txt
- .md
- .json
- .xml
- .yaml
- .yml
- .toml
- .ini

### Special Files
<file>README.md</file>
<metadata>
path: README.md
size: 2993 bytes
</metadata>

[![CodeQL Advanced](https://github.com/ngmisl/llmstxt/actions/workflows/codeql.yml/badge.svg)](https://github.com/ngmisl/llmstxt/actions/workflows/codeql.yml)

# llmstxt

A Python tool for compressing and organizing code files into a single, LLM-friendly text file. This tool is designed to help prepare codebases for analysis by Large Language Models by removing unnecessary content while preserving important semantic information.

## Features

- **Smart Code Compression**
  - Preserves docstrings and important comments
  - Removes redundant whitespace and formatting
  - Maintains code structure and readability
  - Handles multiple programming languages

- **Language Support**
  - Python (with AST-based compression)
  - JavaScript
  - Java
  - C/C++
  - Shell scripts
  - HTML/CSS
  - Configuration files (JSON, YAML, TOML, INI)
  - Markdown

- **LLM-Friendly Output**
  - XML-style semantic markers
  - File metadata and type information
  - Organized imports section
  - Clear file boundaries
  - Consistent formatting

## Installation

This project uses [uv](https://github.com/astral-sh/uv) for dependency management.

```bash
# Install uv if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install the package and its dependencies
uv pip install .

# For development
uv pip install -e ".[dev]"
```

## Usage

```bash
# Generate llms.txt from current directory
python llms.py
```

The script will:

1. Scan the current directory recursively
2. Process files according to .gitignore rules
3. Generate `llms.txt` with compressed content

## Output Format

The generated `llms.txt` file follows this structure:

```python
# Project: llmstxt

## Project Structure
This file contains the compressed and processed contents of the project.

### File Types
- .py
- .js
- .java
...

<file>src/main.py</file>
<metadata>
path: src/main.py
type: py
size: 1234 bytes
</metadata>

<imports>
import ast
from typing import Optional
</imports>

<code lang='python'>
def example():
    """Docstring preserved."""
    return True
</code>

<file>src/utils.js</file>
<metadata>
path: src/utils.js
type: js
size: 567 bytes
</metadata>

<code lang='javascript'>
function helper() {
  return true;
}
</code>
```

## Configuration

The tool can be configured through function parameters:

```python
generate_llms_txt(
    output_file="llms.txt",      # Output filename
    max_file_size=100 * 1024,    # Max file size (100KB)
    allowed_extensions=(         # Supported file types
        ".py", ".js", ".java",
        ".c", ".cpp", ".h", ".hpp",
        ".sh", ".txt", ".md",
        ".json", ".xml", ".yaml",
        ".yml", ".toml", ".ini"
    )
)
```

## Development

Requirements:

- Python 3.8+
- [uv](https://github.com/astral-sh/uv) for dependency management

```bash
# Install dev dependencies
uv pip install -e ".[dev]"

# Run type checking
mypy llms.py

# Run linting and formatting
ruff check .
ruff format .
```

## License

MIT License - See LICENSE file for details


<file>LICENSE</file>
<metadata>
path: LICENSE
size: 1063 bytes
</metadata>

MIT License

Copyright (c) 2024 ngmisl

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


<file>pyproject.toml</file>
<metadata>
path: pyproject.toml
type: toml
size: 1602 bytes
</metadata>

<content type='toml'>
[project]
name = "llmstxt"
version = "0.1.0"
description = "A tool to compress and process code files into a single text file"
requires-python = ">=3.8"
dependencies = [
    "gitignore-parser",
    "astroid",
]

[project.optional-dependencies]
dev = [
    "mypy",
    "ruff",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
check_untyped_defs = true
strict = true

[tool.ruff]
line-length = 88
target-version = "py38"

[tool.ruff.lint]
select = ["E", "F", "I", "B", "W", "C90"]
ignore = []

# Exclude a variety of commonly ignored directories.
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
]

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = "^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$"

[tool.ruff.lint.mccabe]
max-complexity = 15

[tool.ruff.lint.isort]
known-first-party = ["llmstxt"]

[tool.ruff.lint.per-file-ignores]
"llms.py" = ["C901"]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.black]
line-length = 88
target-version = ['py38']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88

</content>

<file>renovate.json</file>
<metadata>
path: renovate.json
type: json
size: 114 bytes
</metadata>

<content type='json'>
{
  "$schema": "https://docs.renovatebot.com/renovate-schema.json",
  "extends": [
    "config:recommended"
  ]
}

</content>

<file>llms.py</file>
<metadata>
path: llms.py
type: py
size: 12329 bytes
</metadata>

<imports>
import ast
import pathlib
import re
from typing import Optional, Sequence, cast
import astroid  # type: ignore
from gitignore_parser import parse_gitignore  # type: ignore
    """Removes multiple blank lines from text files."""
    """Extracts code blocks from Markdown and compresses them."""
        outfile.write("The following file types are included:\n")
        # Include README and LICENSE with metadata
                                        "import ",
                                        "from ",
                                        "require",
                                        "include",
</imports>

<code lang='py'>
import ast
import pathlib
import re
from typing import Optional, Sequence, cast
import astroid
from gitignore_parser import parse_gitignore

def compress_python_code(content: str) -> str:
    """Compress Python code while preserving docstrings."""
    try:
        parsed_ast = ast.parse(content)
    except SyntaxError:
        return content

    class RemoveCommentsAndDocstrings(ast.NodeTransformer):

        def visit_FunctionDef(self, node: ast.FunctionDef) -> Optional[ast.FunctionDef]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def visit_ClassDef(self, node: ast.ClassDef) -> Optional[ast.ClassDef]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def visit_Module(self, node: ast.Module) -> Optional[ast.Module]:
            if ast.get_docstring(node) is not None and node.body and isinstance(node.body[0], ast.Expr) and isinstance(node.body[0].value, ast.Constant):
                docstring = node.body[0]
                node.body = [docstring] + [n for n in map(self.visit, node.body[1:]) if n is not None]
                return node
            node.body = [n for n in map(self.visit, node.body) if n is not None]
            return node

        def generic_visit(self, node: ast.AST) -> ast.AST:
            if isinstance(node, ast.Expr) and isinstance(node.value, ast.Constant):
                return ast.Pass()
            return super().generic_visit(node)
    try:
        transformer = RemoveCommentsAndDocstrings()
        cleaned_ast = transformer.visit(parsed_ast)
        ast.fix_missing_locations(cleaned_ast)
        try:
            source = ast.unparse(cleaned_ast)
        except (AttributeError, TypeError):
            source = cast(str, astroid.parse(ast.dump(cleaned_ast)).as_string())
        lines = source.split('\n')
        cleaned_lines = []
        for line in lines:
            if line.strip() != 'pass':
                cleaned_lines.append(line)
        compressed_code = '\n'.join(cleaned_lines)
        compressed_code = re.sub('\\n\\s*\\n\\s*\\n', '\n\n', compressed_code)
        return compressed_code
    except Exception as e:
        print(f'Warning: Error compressing Python code: {e}')
        return content

def compress_code_content(content: str, file_extension: str) -> str:
    """Compress code content based on the file extension."""
    if file_extension in ('.py', '.pyi'):
        return compress_python_code(content)
    return basic_compress(content, file_extension)

def basic_compress(content: str, file_extension: str) -> str:
    """Basic compression: remove comments and multiple blank lines."""
    lines = content.split('\n')
    cleaned_lines = []
    for line in lines:
        if file_extension in ('.py', '.sh'):
            line = re.sub('#.*$', '', line)
        elif file_extension in ('.js', '.java', '.c', '.cpp', '.h', '.hpp'):
            line = re.sub('//.*$', '', line)
        cleaned_lines.append(line)
    content = '\n'.join(cleaned_lines)
    return re.sub('\\n\\s*\\n\\s*\\n', '\n\n', content)

def compress_text_content(content: str) -> str:
    """Removes multiple blank lines from text files."""
    return re.sub('\\n\\s*\\n\\s*\\n', '\n\n', content)

def compress_markdown_content(content: str) -> str:
    """Extracts code blocks from Markdown and compresses them."""
    parts = re.split('(```\\w*\\n.*?\\n```)', content, flags=re.DOTALL)
    compressed_parts = []
    for part in parts:
        if part.startswith('```'):
            lang_match = re.match('```(\\w*)\\n', part)
            lang = lang_match.group(1) if lang_match else ''
            code = re.sub('```\\w*\\n(.*)\\n```', '\\1', part, flags=re.DOTALL)
            if lang in ('python', 'py'):
                code = compress_python_code(code)
            else:
                code = compress_text_content(code)
            compressed_parts.append(f'```{lang}\n{code}\n```')
        else:
            compressed_parts.append(compress_text_content(part))
    return ''.join(compressed_parts)

def generate_llms_txt(output_file: str='llms.txt', allowed_extensions: Sequence[str]=('.py', '.js', '.html', '.css', '.java', '.c', '.cpp', '.h', '.hpp', '.sh', '.txt', '.md', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini'), max_file_size: int=100 * 1024) -> None:
    """
    Generates a compressed llms.txt file optimized for LLM/AI consumption.

    Args:
        output_file: Name of the output file
        allowed_extensions: Tuple of file extensions to process
        max_file_size: Maximum file size in bytes to process
    """
    current_dir: pathlib.Path = pathlib.Path('.')
    gitignore_path: pathlib.Path = current_dir / '.gitignore'
    matches = parse_gitignore(gitignore_path) if gitignore_path.exists() else None
    with open(output_file, 'w', encoding='utf-8') as outfile:
        outfile.write('# Project: llmstxt\n\n')
        outfile.write('## Project Structure\n')
        outfile.write('This file contains the compressed and processed contents of the project.\n\n')
        outfile.write('### File Types\n')
        outfile.write('The following file types are included:\n')
        outfile.write(''.join([f'- {ext}\n' for ext in allowed_extensions]))
        outfile.write('\n### Special Files\n')
        for special_file in ['README.md', 'LICENSE', 'LICENSE.txt']:
            special_path: pathlib.Path = current_dir / special_file
            if special_path.exists():
                outfile.write(f'<file>{special_file}</file>\n')
                outfile.write('<metadata>\n')
                outfile.write(f'path: {special_file}\n')
                outfile.write(f'size: {special_path.stat().st_size} bytes\n')
                outfile.write('</metadata>\n\n')
                with open(special_path, 'r', encoding='utf-8', errors='replace') as infile:
                    special_content: str = infile.read()
                    outfile.write(special_content + '\n\n')
        for file in current_dir.rglob('*'):
            if file.is_file() and file.suffix.lower() in allowed_extensions and (not (matches and matches(str(file.relative_to(current_dir))))) and (file.name not in ['README.md', 'LICENSE', 'LICENSE.txt', output_file]):
                if file.stat().st_size > max_file_size:
                    print(f'Skipping {file} as it exceeds the maximum file size.')
                    continue
                relative_path: pathlib.Path = file.relative_to(current_dir)
                outfile.write(f'<file>{relative_path}</file>\n')
                outfile.write('<metadata>\n')
                outfile.write(f'path: {relative_path}\n')
                outfile.write(f'type: {file.suffix.lstrip('.')}\n')
                outfile.write(f'size: {file.stat().st_size} bytes\n')
                outfile.write('</metadata>\n\n')
                try:
                    with open(file, 'r', encoding='utf-8', errors='replace') as infile:
                        raw_content: str = infile.read()
                        if file.suffix.lower() in ('.py', '.js', '.java'):
                            outfile.write('<imports>\n')
                            import_lines = [line for line in raw_content.split('\n') if any((imp in line.lower() for imp in ['import ', 'from ', 'require', 'include']))]
                            if import_lines:
                                outfile.write('\n'.join(import_lines) + '\n')
                            outfile.write('</imports>\n\n')
                        if file.suffix.lower() in ('.py', '.js', '.java', '.c', '.cpp', '.h', '.hpp', '.sh'):
                            code_content: str = compress_code_content(raw_content, file.suffix.lower())
                            language: str = file.suffix.lstrip('.')
                            outfile.write(f"<code lang='{language}'>\n{code_content}\n</code>\n\n")
                        elif file.suffix.lower() in ('.txt', '.json', '.xml', '.yaml', '.yml', '.toml', '.ini'):
                            text_content: str = compress_text_content(raw_content)
                            outfile.write(f"<content type='{file.suffix.lstrip('.')}'>\n")
                            outfile.write(f'{text_content}\n')
                            outfile.write('</content>\n\n')
                        elif file.suffix.lower() == '.md':
                            md_content: str = compress_markdown_content(raw_content)
                            outfile.write('<markdown>\n')
                            outfile.write(f'{md_content}\n')
                            outfile.write('</markdown>\n\n')
                        else:
                            outfile.write(f"<content type='{file.suffix.lstrip('.')}'>\n")
                            outfile.write(f'{raw_content}\n')
                            outfile.write('</content>\n\n')
                except Exception as e:
                    outfile.write(f'<error>Error processing {relative_path}: {e}</error>\n\n')
if __name__ == '__main__':
    output_filename: str = 'llms.txt'
    generate_llms_txt(output_filename)
    print(f'{output_filename} generated successfully in the current directory!')
</code>

<file>.vscode/settings.json</file>
<metadata>
path: .vscode/settings.json
type: json
size: 513 bytes
</metadata>

<content type='json'>
{
  "cSpell.words": [
    "celerybeat",
    "Connor",
    "cython",
    "direnv",
    "dmypy",
    "docstrings",
    "htmlcov",
    "infile",
    "ipynb",
    "isort",
    "llms",
    "llmstxt",
    "mccabe",
    "mkdocs",
    "mypy",
    "nosetests",
    "Pipfile",
    "pybuilder",
    "pycache",
    "pyflow",
    "pypa",
    "pypackages",
    "pyrightconfig",
    "pytest",
    "pytype",
    "ropeproject",
    "Scrapy",
    "sdist",
    "Spyder",
    "spyderproject",
    "spyproject",
    "webassets"
  ]
}

</content>

<file>.venv/bin/activate_this.py</file>
<metadata>
path: .venv/bin/activate_this.py
type: py
size: 2390 bytes
</metadata>

<imports>
# included in all copies or substantial portions of the Software.
import runpy
from __future__ import annotations
import os
import site
import sys
    msg = "You must use import runpy; runpy.run_path(this_file)"
    raise AssertionError(msg) from exc
base = bin_dir[: -len("bin") - 1]  # strip away the bin part from the __file__, plus the path separator
# add the virtual environments libraries to the host python import mechanism
</imports>

<code lang='py'>
"""
Activate virtualenv for current interpreter:

import runpy
runpy.run_path(this_file)

This can be used when you must use an existing Python interpreter, not the virtualenv bin/python.
"""
from __future__ import annotations
import os
import site
import sys
try:
    abs_file = os.path.abspath(__file__)
except NameError as exc:
    msg = 'You must use import runpy; runpy.run_path(this_file)'
    raise AssertionError(msg) from exc
bin_dir = os.path.dirname(abs_file)
base = bin_dir[:-len('bin') - 1]
os.environ['PATH'] = os.pathsep.join([bin_dir, *os.environ.get('PATH', '').split(os.pathsep)])
os.environ['VIRTUAL_ENV'] = base
os.environ['VIRTUAL_ENV_PROMPT'] = 'llmstxt' or os.path.basename(base)
prev_length = len(sys.path)
for lib in '../lib/python3.12/site-packages'.split(os.pathsep):
    path = os.path.realpath(os.path.join(bin_dir, lib))
    site.addsitedir(path)
sys.path[:] = sys.path[prev_length:] + sys.path[0:prev_length]
sys.real_prefix = sys.prefix
sys.prefix = base
</code>

<file>.github/workflows/codeql.yml</file>
<metadata>
path: .github/workflows/codeql.yml
type: yml
size: 4301 bytes
</metadata>

<content type='yml'>
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL Advanced"

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  schedule:
    - cron: '26 7 * * 5'

jobs:
  analyze:
    name: Analyze (${{ matrix.language }})
    # Runner size impacts CodeQL analysis time. To learn more, please see:
    #   - https://gh.io/recommended-hardware-resources-for-running-codeql
    #   - https://gh.io/supported-runners-and-hardware-resources
    #   - https://gh.io/using-larger-runners (GitHub.com only)
    # Consider using larger runners or machines with greater resources for possible analysis time improvements.
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      # required for all workflows
      security-events: write

      # required to fetch internal or private CodeQL packs
      packages: read

      # only required for workflows in private repositories
      actions: read
      contents: read

    strategy:
      fail-fast: false
      matrix:
        include:
        - language: python
          build-mode: none
        # CodeQL supports the following values keywords for 'language': 'c-cpp', 'csharp', 'go', 'java-kotlin', 'javascript-typescript', 'python', 'ruby', 'swift'
        # Use `c-cpp` to analyze code written in C, C++ or both
        # Use 'java-kotlin' to analyze code written in Java, Kotlin or both
        # Use 'javascript-typescript' to analyze code written in JavaScript, TypeScript or both
        # To learn more about changing the languages that are analyzed or customizing the build mode for your analysis,
        # see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning.
        # If you are analyzing a compiled language, you can modify the 'build-mode' for that language to customize how
        # your codebase is analyzed, see https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/codeql-code-scanning-for-compiled-languages
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v3
      with:
        languages: ${{ matrix.language }}
        build-mode: ${{ matrix.build-mode }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality

    # If the analyze step fails for one of the languages you are analyzing with
    # "We were unable to automatically build your code", modify the matrix above
    # to set the build mode to "manual" for that language. Then modify this step
    # to build your code.
    # ℹ️ Command-line programs to run using the OS shell.
    # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun
    - if: matrix.build-mode == 'manual'
      shell: bash
      run: |
        echo 'If you are using a "manual" build mode for one or more of the' \
          'languages you are analyzing, replace this with the commands to build' \
          'your code, for example:'
        echo '  make bootstrap'
        echo '  make release'
        exit 1

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v3
      with:
        category: "/language:${{matrix.language}}"

</content>

